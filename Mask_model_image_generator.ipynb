{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "from tensorflow.keras import utils\n",
    "from imgaug import augmenters as iaa\n",
    "# 進度條模組\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5種類\n",
    "class_names = ['normal'  , 'error_iron' , 'error_ear' , 'noiron' , 'noearing']\n",
    "# 7種類\n",
    "# class_names = ['normal' , 'error_ear' , 'error_iron' , 'error_solder' , 'noearing' , 'noiron' , 'NG']\n",
    "\n",
    "class_names_label = {class_name:i for i,class_name in enumerate(class_names)}\n",
    "print(class_names_label)\n",
    "\n",
    "num_class = len(class_names)\n",
    "\n",
    "IMAGE_SIZE = (240 , 130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "#     5 label\n",
    "    datasets = ['E:/mask_data/New_mask_data_5/train' , 'E:/mask_data/New_mask_data_5/validation' , 'E:/mask_data/New_mask_data_5/test']\n",
    "#     8 label\n",
    "#     datasets = ['E:/mask_data/New_mask_data/train' , 'E:/mask_data/New_mask_data/validation' , 'E:/mask_data/New_mask_data/test']\n",
    "    output = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        print('Loading{}'.format(dataset))\n",
    "        if dataset == datasets[0]:\n",
    "            for folder in os.listdir(dataset):\n",
    "                label = class_names_label[folder]\n",
    "\n",
    "                for file in (os.listdir(os.path.join(dataset , folder))):\n",
    "\n",
    "\n",
    "                    img_path = os.path.join(os.path.join(dataset , folder), file)\n",
    "\n",
    "                    image = cv2.imread(img_path)\n",
    "                    image = cv2.cvtColor(image , cv2.COLOR_BGR2GRAY)\n",
    "                    image = cv2.resize(image , IMAGE_SIZE)\n",
    "                    images.append(image)\n",
    "                    labels.append(label)\n",
    "\n",
    "                    flipper = iaa.Fliplr(1.0) #水平翻轉機率==1.0\n",
    "                    image_f = flipper.augment_image(image) \n",
    "                    images.append(image_f)\n",
    "                    labels.append(label)\n",
    "\n",
    "                    vflipper = iaa.Flipud(1.0) #垂直翻轉機率40%\n",
    "                    image_v = vflipper.augment_image(image) \n",
    "                    images.append(image_v)\n",
    "                    labels.append(label)\n",
    "\n",
    "            images = np.array(images , dtype = 'float32')\n",
    "            labels = np.array(labels , dtype = 'int32')\n",
    "            labels = utils.to_categorical(labels , num_class , dtype = 'int32')\n",
    "            output.append((images,labels))\n",
    "        else:\n",
    "            for folder in os.listdir(dataset):\n",
    "                label = class_names_label[folder]\n",
    "\n",
    "                for file in (os.listdir(os.path.join(dataset , folder))):\n",
    "\n",
    "\n",
    "                    img_path = os.path.join(os.path.join(dataset , folder), file)\n",
    "\n",
    "                    image = cv2.imread(img_path)\n",
    "                    image = cv2.cvtColor(image , cv2.COLOR_BGR2GRAY)\n",
    "                    image = cv2.resize(image , IMAGE_SIZE)\n",
    "                    images.append(image)\n",
    "                    labels.append(label)\n",
    "\n",
    "            images = np.array(images , dtype = 'float32')\n",
    "            labels = np.array(labels , dtype = 'int32')\n",
    "            labels = utils.to_categorical(labels , num_class , dtype = 'int32')\n",
    "            output.append((images,labels))\n",
    "        \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "(train_images, train_labels), (val_images, val_labels), (test_images, test_labels) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle           \n",
    "\n",
    "train_images, train_labels = shuffle(train_images, train_labels, random_state=25)\n",
    "val_images, val_labels = shuffle(val_images, val_labels, random_state=25)\n",
    "test_images, test_labels = shuffle(test_images, test_labels, random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many training and testing example do we have\n",
    "# check the size of image\n",
    "\n",
    "n_train = train_labels.shape[0]\n",
    "n_val = val_labels.shape[0]\n",
    "n_test = test_labels.shape[0]\n",
    "\n",
    "print('Number of training example : {}'.format(n_train))\n",
    "print('Number of training example : {}'.format(n_val))\n",
    "print('Number of testing example : {}'.format(n_test))\n",
    "print('Each image is of size : {}'.format(IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "val_images = val_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 灰階專用\n",
    "train_images = tf.expand_dims(train_images, axis=-1)\n",
    "val_images = tf.expand_dims(val_images, axis=-1)\n",
    "test_images = tf.expand_dims(test_images, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# batch_size = 20\n",
    "\n",
    "# datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip = True,rescale = 1/255)\n",
    "# test_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "\n",
    "# Train_batches = datagen.flow(train_images , train_labels , batch_size = batch_size)\n",
    "\n",
    "# Val_batches = test_datagen.flow(val_images , val_labels , batch_size = batch_size)\n",
    "\n",
    "# Test_batches = test_datagen.flow(test_images , test_labels , batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Train_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Train_batches.n % Train_batches.batch_size ==0:\n",
    "#     step_size_train=Train_batches.n//Train_batches.batch_size\n",
    "# else:\n",
    "#     step_size_train=Train_batches.n//Train_batches.batch_size + 1\n",
    "# print(step_size_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Val_batches.n % Val_batches.batch_size ==0:\n",
    "#     step_size_val=Val_batches.n//Val_batches.batch_size\n",
    "# else:\n",
    "#     step_size_val=Val_batches.n//Val_batches.batch_size + 1\n",
    "# print(step_size_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Flatten ,Dropout ,BatchNormalization\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(130 ,240 ,1))\n",
    "\n",
    "#Model\n",
    "model = tf.keras.applications.EfficientNetB7(include_top = False , weights = None ,input_tensor = input_layer, classifier_activation = 'softmax')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = model.output\n",
    "last_layer = tf.keras.layers.GlobalAveragePooling2D()(last_layer)\n",
    "last_layer = tf.keras.layers.Dropout(0.2)(last_layer)\n",
    "out = Dense(num_class, activation='softmax', name='softmax')(last_layer)\n",
    "model = Model(input_layer, out,name = 'EfficientNetB7-5label')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import time\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "Adam = tf.keras.optimizers.Adam(learning_rate=0.00025,name='Adam')\n",
    "\n",
    "# earlystop = EarlyStopping(monitor='val_loss', mode='min', \n",
    "#                           verbose=1, patience=5, min_delta=0.0001)\n",
    "\n",
    "filepath = 'Mask-5label-EfficientNetB7.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=1, verbose=1, mode='min', min_lr=0.0001)\n",
    "\n",
    "# callback_list = [checkpoint , reduce_lr , earlystop]\n",
    "callback_list = [checkpoint , reduce_lr]\n",
    "# callback_list = [reduce_lr , earlystop]\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam,metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=15, verbose=1, validation_data = val_dataset,callbacks = callback_list)\n",
    "\n",
    "# history = model.fit(Train_batches , steps_per_epoch=step_size_train , \n",
    "#                               validation_data = Val_batches , validation_steps = step_size_val ,\n",
    "#                              epochs=80 , callbacks = callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "print('Time：'+str(math.floor(end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_loss(history):\n",
    "    \"\"\"\n",
    "        Plot the accuracy and the loss during the training of the nn.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(221)\n",
    "    plt.plot(history.history['accuracy'],'b-', label = \"acc\")\n",
    "    plt.plot(history.history['val_accuracy'], 'r-', label = \"val_acc\")\n",
    "    plt.title(\"train_acc vs val_acc\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss function\n",
    "    plt.subplot(222)\n",
    "    plt.plot(history.history['loss'],'b-', label = \"loss\")\n",
    "    plt.plot(history.history['val_loss'], 'r-', label = \"val_loss\")\n",
    "    plt.title(\"train_loss vs val_loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_accuracy_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(filepath)\n",
    "\n",
    "predictions = model.predict(test_images)\n",
    "# predictions = model.predict_generator(Test_batches)     \n",
    "\n",
    "pred_labels = np.argmax(predictions, axis = 1) \n",
    "pred_labels = np.array(pred_labels,dtype = 'int32')\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.argmax(test_labels, axis = 1) \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    title='Mask-5label-EfficientNetB7 confusion matrix'\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "#     plt.savefig('E://Mask_confusion//Mask-EfficientNetB0.jpg')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "confusion_mtx = confusion_matrix(labels, pred_labels)\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(num_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_images, test_labels, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "fail = 0\n",
    "\n",
    "for i in range(208):\n",
    "    count+=1\n",
    "    if pred_labels[i]!=labels[i]:\n",
    "        fail+=1\n",
    "print(count)\n",
    "print(fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in pred_labels:\n",
    "    c+=1\n",
    "print(c)\n",
    "64+28"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
